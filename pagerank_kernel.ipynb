{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "**Some context to start with!**\n-------------------------------\n\n\nPageRank is an ingenious algorithm, developed by *Larry and Sergey*, arguably the biggest game-changer in the this world that we live in! Pagerank basically ranks the nodes in a graph structure, based on the linkages between them. An edge shared with an important node makes you important as well, a link with the spam node makes you spam too!\n\n## Why here? ##\n\nIn our context, every node is a question in our dataset and an edge represents a question pair. More often than not, an edge is shared by somehow related questions(topically), but may not be semantically equivalent -- This edge is however useful to visualize clusters of topics, and importance of certain nodes.\n\nThus, if a question is paired with a rather famous(higher pageranked) question, the question becomes relevant in itself.\n\nNote: The implementation of a Pagerank in this context was inspired by this [discussion][1] from Krzysztof Dziedzic\n\n\n  [1]: https://www.kaggle.com/c/quora-question-pairs/discussion/33664",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's get started, shall we!\n----------------------------",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\n\ndf_train = pd.read_csv('../input/train.csv').fillna(\"\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The small function below computes a dictionary of questions, where each key-value pair is a question and its neighboring questions(in a list). This is necessary before we get along with calculating each question's pagerank! ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Generating a graph of Questions and their neighbors\ndef generate_qid_graph_table(row):\n\n    hash_key1 = row[\"qid1\"]\n    hash_key2 = row[\"qid2\"]\n        \n    qid_graph.setdefault(hash_key1, []).append(hash_key2)\n    qid_graph.setdefault(hash_key2, []).append(hash_key1)\n\nqid_graph = {}\ndf_train.apply(generate_qid_graph_table, axis = 1); #You should apply this on df_test too. Avoiding here on the kernel.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Cut to the chase! ##\n\nWithout getting into a lot of details, pagerank of a node is defined as the sum of a certain ratio of all its neighbors -- a complete dependence on adjacent vertices. The ratio is basically, the pagerank of the neighbor divided by the degree of the neighbor(edges incident on it). \n\nMathematically speaking,\nPR(n) = PR(n1)/num_neighbors(n1) + ... + PR(n_last)/num_neighbors(n_last)\n\nHowever, a damping factor is also induced in this formula, so as to account for how often the edge is to be taken(within the context of a random surfer on the web)\n\nThus, \n**PR(n) = (1-d)/N + d*(PR(n1)/num_neighbors(n1) + ... + PR(n_last)/num_neighbors(n_last))**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def pagerank():\n\n    MAX_ITER = 20 #Let me know if you find an optimal iteration number!\n    d = 0.85\n    \n    #Initializing -- every node gets a uniform value!\n    pagerank_dict = {i:1/len(qid_graph) for i in qid_graph}\n    num_nodes = len(pagerank_dict)\n    \n    for iter in range(0, MAX_ITER):\n        \n        for node in qid_graph:    \n            local_pr = 0\n            \n            for neighbor in qid_graph[node]:\n                local_pr += pagerank_dict[neighbor]/len(qid_graph[neighbor])\n            \n            pagerank_dict[node] = (1-d)/num_nodes + d*local_pr\n\n    return pagerank_dict\n\npagerank_dict = pagerank()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We initially begin with a uniform pagerank value to all nodes, and with every iteration the pageranks begin to converge. You can also introduce a minimum difference between iterations to ensure convergence.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Getting the pageranks ##\n\nFinally, a getter function to concatenate the features with the rest of the dataframe.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def get_pagerank_value(row):\n    return pd.Series({\n        \"q1_pr\": pagerank_dict[row[\"qid1\"]],\n        \"q2_pr\": pagerank_dict[row[\"qid2\"]]\n    })\n\npagerank_feats_train = df_train.apply(get_pagerank_value, axis = 1);",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Result ##\n\nThese features gave me a slight 0.002 bump on a 100-nround xgboost, not a magic feature by any means ;)\n\nFork away :)",
      "metadata": {}
    }
  ]
}